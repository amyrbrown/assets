Hi everyone,

I've spoken to a couple of engineers here at Mozilla about how to
capture the process and the review, and to Prof. Marian Petre at the
Open University about assessment --- she has a world-class reputation
doing empirical research in software engineering, particularly
observational work like this.  Based on those discussions, I'd like to
propose that:

1. Mozilla sets up an instance of a simple code review tool called
   ReviewBoard at http://review.software-carpentry.org

   * We'll require passwords to log in to ReviewBoard, so that only
     participants in the pilot will be able to see stuff.  However,
     authors and reviewers _will_ be able to see each other's code and
     reviews.

   * We will brainstorm with Marian about what data we ought to collect
     that ReviewBoard doesn't already gather.  If there is any, and if
     we can throw together a small plugin to collect it, we'll do that
     before inviting submissions.

2. PLOS contacts the authors of 40-50 papers published in the last 12
   months and invites them to take part in the pilot (wording at the
   end of this message).  We'll start accepting code and doing reviews
   in the first week of August.

3. Authors email their code to review@software-carpentry.org, and we
   add it to a version control repository.  (Normally, we'd just point
   ReviewBoard at the authors' code repository, but if we require that
   for the pilot, we'll bias selection toward the most competent
   participants.)

4. We match code to volunteer reviewers.  We'll aim for first-come,
   first-served, but it will also depend on which reviewers feel they
   can understand what.

5. As each review is completed, we mail authors to let them know where
   the review is, and ask for their feedback on the review.  (They can
   provide this by adding comments of their own through ReviewBoard,
   or by sending us notes by email --- we prefer the former, since
   it's the normal workflow, but we'll accept the latter for the
   pilot.)

6. We'll turn off the taps at the end of August, then do interviews
   with authors and reviewers in the first half of September by email
   and phone to supplement whatever we've been able to collect through
   ReviewBoard.

------------------------------------------------------------

Dear author,

In partnership with the Mozilla Science Lab, the Public Library of
Science (PLOS) is launching a pilot project to explore what code
review could and should look like for research.  During the next three
months, a dozen Mozilla developers will spent a few hours each
reviewing three or four small programs used to produce results in
papers published over the past 12 months.  From this, we hope to
learn:

1. How much scientific software can be reviewed by non-specialists,
   and how often is domain expertise required?

2. How much effort does this take compared to reviews of other kinds
   of software, and to reviews of papers themselves?

3. How useful do scientists find these reviews?

If you would like to take part in this program, we invite you to
submit any piece of software that:

* you wrote yourself to produce a result in a recently-published
  paper;

* is 50-200 lines long; and

* is written in Perl, Python, R, C/C++, or Java.

Please note that this is only a pilot: the code you submit, and the
reviews of it, will only be accessible to participants in the program,
and the review will not affect the status of your published paper in
any way.  If your code is reviewed, we will ask you to subsequently
give us up to half an hour of your time for an interview by email or
phone so that we can learn more about how and whether to take this
forward.

If you'd like to take part in this pilot study, please contact us by
email at science-code-review@software-carpentry.org.
